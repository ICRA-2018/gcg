{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation\n",
    "\n",
    "[Arxiv link](https://arxiv.org/abs/1709.10489)\n",
    "\n",
    "<b>Abstract</b>: Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and N-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. \n",
    "\n",
    "Click below to view video\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/vgiW0HlQWVE/0.jpg)](https://www.youtube.com/watch?v=vgiW0HlQWVE)\n",
    "\n",
    "---\n",
    "# Code\n",
    "\n",
    "This repository contains the code to run the simulation experiments. The main code is in [sandbox/gkahn/gcg](https://github.com/gkahn13/gcg/tree/gcg_release/sandbox/gkahn/gcg), while the rllab code was used for infrastructure purposes (e.g., running experiments on EC2).\n",
    "\n",
    "---\n",
    "### Installation\n",
    "\n",
    "Clone the repository and add it to your PYTHONPATH\n",
    "\n",
    "Install [Anaconda using the Python 2.7 installer](https://www.anaconda.com/download/).\n",
    "\n",
    "We will always assume the current directory is [sandbox/gkahn/gcg](https://github.com/gkahn13/gcg/tree/gcg_release/sandbox/gkahn/gcg). Create a new Anaconda environment and activate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ CONDA_SSL_VERIFY=false conda env create -f environment.yml\n",
    "$ source activate gcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Panda3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ pip install --pre --extra-index-url https://archive.panda3d.org/ panda3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the simulation speed by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ nvidia-settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And disabling \"Sync to VBLank\" under \"OpenGL Settings\"\n",
    "\n",
    "---\n",
    "### Simulation environment\n",
    "\n",
    "To drive in the simulation environment, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ python envs/rccar/square_cluttered_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands are\n",
    "- [ w ] forward\n",
    "- [ x ] backward\n",
    "- [ a ] left\n",
    "- [ d ] right\n",
    "- [ s ] stop\n",
    "- [ r ] reset\n",
    "\n",
    "---\n",
    "### Yaml experiment configuration files\n",
    "\n",
    "The [yamls](https://github.com/gkahn13/gcg/tree/gcg_release/sandbox/gkahn/gcg/yamls) folder contains experiment configuration files for [Double Q-learning](https://github.com/gkahn13/gcg/tree/gcg_release/sandbox/gkahn/gcg/yamls/dql.yaml) , [5-step Double Q-learning](https://github.com/gkahn13/gcg/tree/gcg_release/sandbox/gkahn/gcg/yamls/nstep_dql.yaml) , and [our approach](https://github.com/gkahn13/gcg/tree/gcg_release/sandbox/gkahn/gcg/yamls/ours.yaml).\n",
    "\n",
    "These yaml files can be adapted to form alternative instantiations of the generalized computation graph. Please see the example yaml files for detailed descriptions.\n",
    "\n",
    "---\n",
    "### Running the code\n",
    "\n",
    "To run our approach, execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ python run_exp.py --exps ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be stored in the gcg/data folder.\n",
    "\n",
    "You can run other yaml files by replacing \"ours\" with the desired yaml file name (e.g., \"dql\" or \"nstep_dql\")\n",
    "\n",
    "---\n",
    "### References\n",
    "\n",
    "Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, Sergey Levine. \"Self-supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation.\" \t[arXiv:1709.10489](https://arxiv.org/abs/1709.10489)"
   ]
  }
 ],

 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
